services:
  caddy:
    image: caddy:2
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config

  api:
    image: ghcr.io/aniketbansod/ai-project-backend:prod-latest
    env_file: backend.env
    environment:
      # Ensure internal service-to-service calls target the AI container over the Docker network
      - AI_SERVICE_URL=http://ai:8000
    depends_on:
      - ai
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  worker:
    image: ghcr.io/aniketbansod/ai-project-worker:prod-latest
    env_file: backend.env
    environment:
      # Ensure internal service-to-service calls target the AI container over the Docker network
      - AI_SERVICE_URL=http://ai:8000
    # Make sure the worker container runs the BullMQ worker process, not the API server
    command: ["npm", "run", "start:worker"]
    depends_on:
      - api
      - ai
    restart: unless-stopped

  ai:
    image: ghcr.io/aniketbansod/ai-service:prod-latest
    env_file: ai.env
    environment:
      - FAISS_INDEX_PATH=/app/data/faiss_index.bin
      - FAISS_META_PATH=/app/data/faiss_meta.pkl
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    volumes:
      - ai_index:/app/data
      - hf_cache:/root/.cache/huggingface

volumes:
  ai_index:
  hf_cache:
  caddy_data:
  caddy_config:
